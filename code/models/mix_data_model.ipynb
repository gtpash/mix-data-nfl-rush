{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as kb\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (\n",
    "    Activation, \n",
    "    BatchNormalization, \n",
    "    Concatenate,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    MaxPooling2D\n",
    ")\n",
    "from keras.optimizers import TFOptimizer, Adam, SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(dim):\n",
    "    # define our MLP network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=dim, activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(128, input_dim=dim, activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(64, input_dim=dim, activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # return our model\n",
    "    return model\n",
    "\n",
    "def create_cnn(height, width, depth, filters=(16, 32, 64), regress=False):\n",
    "    # initialize the input shape and channel dimension, assuming\n",
    "    # TensorFlow/channels-last ordering\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1\n",
    "\n",
    "    # define the model input\n",
    "    inputs = Input(shape=inputShape)\n",
    "\n",
    "    # loop over the number of filters\n",
    "    for (i, f) in enumerate(filters):\n",
    "        # if this is the first CONV layer then set the input\n",
    "        # appropriately\n",
    "        if i == 0:\n",
    "            x = inputs\n",
    "            filt = (5,5)\n",
    "        else:\n",
    "            filt = (3,3)\n",
    "        # CONV => RELU => BN => POOL\n",
    "        \n",
    "        x = Conv2D(f, filt, padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # flatten the volume, then FC => RELU => BN => DROPOUT\n",
    "    x = Flatten()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(1024)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    \n",
    "    # apply another FC layer, this one to match the number of nodes\n",
    "    # coming out of the MLP\n",
    "    x = Dense(128)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(64)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    # construct the CNN\n",
    "    model = Model(inputs, x)\n",
    "\n",
    "    # return the CNN\n",
    "    return model\n",
    "\n",
    "def CRPS(yTrue, yPred):\n",
    "    yPred = kb.cumsum(yPred, axis=1)\n",
    "    return kb.mean(kb.sum(kb.square(yPred - yTrue), axis=1)) / 199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=32, imageDim=(100,200),\n",
    "                 nChannels = 4, gsDim = 37, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.imageDim = imageDim\n",
    "        self.gsDim = gsDim\n",
    "        self.nChannels = nChannels\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        Xcombined, yardage = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return Xcombined, yardage\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        Xim = np.empty((self.batch_size, *self.imageDim, self.nChannels))\n",
    "        Xgs = np.empty((self.batch_size, self.gsDim))\n",
    "        yardage = np.empty((self.batch_size,199), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            temp = np.load('data/files/image' + ID + '.npy')\n",
    "            Xim[i,] = temp.astype('float')/128.\n",
    "            Xgs[i,] = np.load('data/files/gameState' + ID + '.npy')\n",
    "            \n",
    "            # Store output\n",
    "            yardage[i,] = np.load('data/files/yardage' + ID + '.npy')\n",
    "\n",
    "        return [Xgs,Xim], yardage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = np.load('data/partition_dict.npy',allow_pickle = 'TRUE').item()\n",
    "batch_size = 64\n",
    "gsDim = 37\n",
    "training_generator = DataGenerator(\n",
    "    partition['train'], \n",
    "    batch_size = batch_size)\n",
    "validation_generator = DataGenerator(\n",
    "    partition['validation'], \n",
    "    batch_size = batch_size)\n",
    "testing_generator = DataGenerator(\n",
    "    partition['test'], \n",
    "    batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = create_mlp(gsDim)\n",
    "cnn = create_cnn(100, 200, 4)\n",
    "\n",
    "x = Concatenate()([mlp.output, cnn.output])\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x = Dense(199, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=[mlp.input, cnn.input], \n",
    "              outputs=x)\n",
    "model.compile(loss=CRPS,\n",
    "              metrics = [CRPS],\n",
    "              optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_json = model.to_json()\n",
    "with open(\"combined.json\", \"w\") as json_file:\n",
    "    json_file.write(combined_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "760/760 [==============================] - 1285s 2s/step - loss: 0.0215 - CRPS: 0.0215 - val_loss: 0.0181 - val_CRPS: 0.0148\n",
      "\n",
      "Epoch 00001: saving model to ./combined/combinedWeights.hdf5\n",
      "Epoch 2/2\n",
      "760/760 [==============================] - 1248s 2s/step - loss: 0.0153 - CRPS: 0.0153 - val_loss: 0.0142 - val_CRPS: 0.0144\n",
      "\n",
      "Epoch 00002: saving model to ./combined/combinedWeights.hdf5\n"
     ]
    }
   ],
   "source": [
    "checkpointer = keras.callbacks.ModelCheckpoint(filepath='./combined/combinedWeights.hdf5', verbose=1, save_best_only=False)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', \n",
    "                   mode='min',\n",
    "                   restore_best_weights=True, \n",
    "                   verbose=1, \n",
    "                   patience=11)\n",
    "es.set_model(model)\n",
    "\n",
    "lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                       factor=0.5,\n",
    "                       patience=5,\n",
    "                       verbose=1,\n",
    "                       mode='min',\n",
    "                       min_delta=0.00001)\n",
    "\n",
    "history = model.fit_generator(generator = training_generator,\n",
    "                   validation_data = validation_generator,\n",
    "                   epochs = 2,\n",
    "                   callbacks = [es, lr, checkpointer],\n",
    "                   verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 54s 334ms/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate_generator(testing_generator, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.015449351631104946, 0.01481718197464943]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save('./combined/comb_test_score.npy',score)\n",
    "np.save('./combined/comb_hist.npy',history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
